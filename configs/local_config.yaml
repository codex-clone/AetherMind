# =========================================================================
# AetherMind Local Training Config
# =========================================================================
# Target: RTX 3050 Ti, 4GB VRAM, 16GB RAM
# All memory optimizations are enabled for this constrained environment.
# =========================================================================

hardware:
  device: "cuda"                    # "cuda" or "cpu"
  precision: "fp16"                 # "fp32", "fp16", "bf16"
  gradient_checkpointing: true      # Trade compute for memory - MUST be true on 4GB
  pin_memory: true                  # Faster CPU to GPU data transfer
  compile_model: false              # torch.compile() - can help but may cause issues

model:
  variant: "nano"                   # "nano" (125M) or "mini" (350M)
  context_length: 1024              # Max sequence length - reduce if OOM
  
training:
  batch_size: 2                     # Physical batch size per step - keep LOW
  gradient_accumulation_steps: 8    # Effective batch size = 2 * 8 = 16
  num_epochs: 3                     # Total training epochs
  learning_rate: 3.0e-4             # Peak learning rate
  weight_decay: 0.1                 # AdamW weight decay
  max_grad_norm: 1.0                # Gradient clipping threshold
  warmup_ratio: 0.02                # 2% of total steps for LR warmup
  lr_scheduler: "cosine"            # "cosine" or "linear" decay
  use_8bit_optimizer: false         # bitsandbytes 8-bit AdamW (Linux only)
  adam_beta1: 0.9                   # Adam first moment decay
  adam_beta2: 0.95                  # Adam second moment decay (0.95 for LLMs)
  adam_epsilon: 1.0e-8              # Adam epsilon for numerical stability

checkpointing:
  save_every_n_steps: 500           # Save checkpoint every N steps
  keep_last_n: 3                    # Delete older checkpoints to save disk
  output_dir: "outputs/checkpoints" # Checkpoint save directory

data:
  num_workers: 2                    # DataLoader workers - keep low on Windows
  max_samples: 200000               # Limit dataset size for faster training
  shuffle: true                     # Shuffle training data each epoch
  
logging:
  log_every_n_steps: 10             # Print metrics every N steps
  log_dir: "outputs/logs"           # Log file directory
  use_rich_ui: true                 # Use Rich terminal dashboard
